{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCceAgu90vQE",
        "outputId": "ddf0c5bf-bff6-4a8d-cd82-2a85a518421b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the MNIST dataset (or any other dataset like HAM 10000)"
      ],
      "metadata": {
        "id": "zwQQfCPi1EkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
      ],
      "metadata": {
        "id": "rovmjDDW0_jO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract two subsets of 600 data points each (without intersection)"
      ],
      "metadata": {
        "id": "bhQyHGAU1v56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_subsets(dataset_train, dataset_test, subset_size):\n",
        "  ind = np.arange(len(dataset_train))\n",
        "  np.random.shuffle(ind)\n",
        "  #ind = ind[: 2 * subset_size]\n",
        "\n",
        "\n",
        "\n",
        "  sub1 = ind[: subset_size]\n",
        "  sub2 = ind[subset_size : 2 * subset_size]\n",
        "  true_sub1 = Subset(dataset_train, sub1)\n",
        "  true_sub2 = Subset(dataset_train, sub2)\n",
        "\n",
        "  sub1_load = DataLoader(true_sub1, batch_size=50, shuffle=True)\n",
        "  sub2_load = DataLoader(true_sub2, batch_size=50, shuffle=True)\n",
        "  dl_train = DataLoader(dataset_train, batch_size=50, shuffle=True)\n",
        "  dl_test = DataLoader(dataset_test, batch_size=50, shuffle=False)\n",
        "\n",
        "  return sub1_load, sub2_load, dl_train, dl_test\n",
        "\n",
        "#get_subsets(mnist_train, mnist_test, 600)"
      ],
      "metadata": {
        "id": "QAN-1uL61z4-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a simple Convolutional Neural Network (2 convolutional layers and 2 dense layers, for example)"
      ],
      "metadata": {
        "id": "-0xUN0uD58Pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(64 * 28 * 28, 128)\n",
        "    self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.relu(self.conv2(self.relu(self.conv1(x))))\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.fc2(self.relu(self.fc1(out)))\n",
        "    return out"
      ],
      "metadata": {
        "id": "zz6wA48x6A_A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a function average_model_parameters(models: iterable, average_weight): iterable that takes a list of models as an argument and returns the weighted average of the parameters of each model."
      ],
      "metadata": {
        "id": "JM1dWkIX9cvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_model_parameters(models, average_weight):\n",
        "  with torch.no_grad():\n",
        "\n",
        "    mp = [list(m.parameters()) for m in models]\n",
        "    #print(mp)\n",
        "    res_amp = []\n",
        "    for i in range(len(mp[0])):\n",
        "      sum = 0\n",
        "      for j, w in enumerate(average_weight):\n",
        "        sum += w * mp[j][i].data\n",
        "      #print(sum)\n",
        "      res_amp.append(sum) #.clone() deleted\n",
        "  return res_amp"
      ],
      "metadata": {
        "id": "1J-Pl1UV9eMj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a function that updates the parameters of a model from a list of values"
      ],
      "metadata": {
        "id": "THkq3V_nBI1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(model, param):\n",
        "  with torch.no_grad():\n",
        "    for i, p in zip(model.parameters(), param):\n",
        "      i.data = p.clone()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    rl = 0.0\n",
        "    truee = 0\n",
        "    total = 0\n",
        "    #other = 0\n",
        "    for i, t in dataloader:\n",
        "        i = i.to(device)\n",
        "        t = t.to(device)\n",
        "        #stop ici, continuer demain, doit faire evaluate (same) + update loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(i)\n",
        "        loss = criterion(outputs, t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        #faire modif\n",
        "        rl += loss.item() * i.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        truee += predicted.eq(t).sum().item()\n",
        "        total += t.size(0)\n",
        "    epoch_l = rl / total\n",
        "    epoch_acc = 100.0 * truee / total\n",
        "    return epoch_l, epoch_acc\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    rl = 0.0\n",
        "    truee = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for i, t in dataloader:\n",
        "            i = i.to(device)\n",
        "            t = t.to(device)\n",
        "            outputs = model(i)\n",
        "            loss = criterion(outputs, t)\n",
        "            rl += loss.item() * i.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            truee += predicted.eq(t).sum().item()\n",
        "            total += t.size(0)\n",
        "    #ajouter conditions\n",
        "    epoch_l = rl / total if total > 0 else 0\n",
        "    epoch_acc = 100.0 * truee / total if total > 0 else 0\n",
        "    return epoch_l, epoch_acc"
      ],
      "metadata": {
        "id": "VNMhmLHdBM_H"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a script/code/function that reproduces Algorithm 1, considering that both models are on your machine. Use an average_weight=[1/2, 1/2]. Reuse the same setup as in the article (50 examples per local batch)"
      ],
      "metadata": {
        "id": "3llFL-yYBqEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def without_common(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001):\n",
        "  sub1_load, sub2_load, dl_train, dl_test = get_subsets(mnist_train, mnist_test, nb_subset)\n",
        "\n",
        "  #Cas without initializing the common parameters donc pas de modèle partagé\n",
        "  model1 = CNN().to(device)\n",
        "  model2 = CNN().to(device)\n",
        "  opti1 = optim.Adam(model1.parameters(), lr=lr)\n",
        "  opti2 = optim.Adam(model2.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(local_epoch):\n",
        "    train_one_epoch(model1, sub1_load, opti1, criterion)\n",
        "    train_one_epoch(model2, sub2_load, opti2, criterion)\n",
        "  moy = average_model_parameters([model1, model2], average_weight)\n",
        "  moy_model = CNN().to(device)\n",
        "  update_parameters(moy_model, moy)\n",
        "  train_l, train_acc = evaluate(moy_model, dl_train, criterion)\n",
        "  test_l, test_acc = evaluate(moy_model, dl_test, criterion)\n",
        "  print(\"For no common init :\")\n",
        "  print(f\"Average Model on Train | Loss : {train_l} | Accuracy : {train_acc}\")\n",
        "  print(f\"Average Model on Test | Loss : {test_l} | Accuracy : {test_acc}\")\n",
        "  return moy_model\n",
        "\n",
        "\n",
        "def with_common(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001):\n",
        "  sub1_load, sub2_load, dl_train, dl_test = get_subsets(mnist_train, mnist_test, nb_subset)\n",
        "\n",
        "  #Cas with initializing the common parameters\n",
        "  model = CNN().to(device)\n",
        "  model1 = copy.deepcopy(model)\n",
        "  model2 = copy.deepcopy(model)\n",
        "  opti1 = optim.Adam(model1.parameters(), lr=lr)\n",
        "  opti2 = optim.Adam(model2.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(local_epoch):\n",
        "    train_one_epoch(model1, sub1_load, opti1, criterion)\n",
        "    train_one_epoch(model2, sub2_load, opti2, criterion)\n",
        "  moy = average_model_parameters([model1, model2], average_weight)\n",
        "  moy_model = CNN().to(device)\n",
        "  update_parameters(moy_model, moy)\n",
        "  train_l, train_acc = evaluate(moy_model, dl_train, criterion)\n",
        "  test_l, test_acc = evaluate(moy_model, dl_test, criterion)\n",
        "  print(\"For no common init :\")\n",
        "  print(f\"Average Model on Train | Loss : {train_l} | Accuracy : {train_acc}\")\n",
        "  print(f\"Average Model on Test | Loss : {test_l} | Accuracy : {test_acc}\")\n",
        "  return moy_model"
      ],
      "metadata": {
        "id": "ZkmrDL1QBwWR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train your models without initializing the common parameters and measure the performance on the entire dataset."
      ],
      "metadata": {
        "id": "PT2h9bB-x1gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_no_init = without_common(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXcs8zHGx5z9",
        "outputId": "06954c1c-1712-4883-f3b1-ca47d6aa7682"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For no common init :\n",
            "Average Model on Train | Loss : 2.100715908805529 | Accuracy : 34.34\n",
            "Average Model on Test | Loss : 2.1001648509502413 | Accuracy : 33.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train your models with the initialization of common parameters and verify that the performance is better."
      ],
      "metadata": {
        "id": "y9aBknkTyM92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_init = with_common(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmYXlWhnyOOL",
        "outputId": "17ad6a7c-3877-473a-86d9-29ae52ea5086"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For no common init :\n",
            "Average Model on Train | Loss : 0.9703225161631902 | Accuracy : 68.135\n",
            "Average Model on Test | Loss : 0.9448229749500752 | Accuracy : 69.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduce the number of data points in each sub-batch. What is the minimum number of data points necessary for the final model to have acceptable performance?"
      ],
      "metadata": {
        "id": "fwryZRXrycJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for b in [50, 25, 10, 5, 3, 2, 1]:\n",
        "  print(f\"Local batch size : {b}\")\n",
        "  model_no_init = without_common(nb_subset=600, local_batch=b, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)\n",
        "  model_init = with_common(nb_subset=600, local_batch=b, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)\n",
        "  print(\"============================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDFSG7yUygMr",
        "outputId": "ac24f398-0296-4039-c2dd-00966a28d761"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local batch size : 50\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.1970659669240318 | Accuracy : 41.87166666666667\n",
            "Average Model on Test | Loss : 2.1942656803131104 | Accuracy : 42.62\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 0.9805917155742645 | Accuracy : 66.07333333333334\n",
            "Average Model on Test | Loss : 0.9703756673634052 | Accuracy : 66.45\n",
            "============================================\n",
            "Local batch size : 25\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.194576689998309 | Accuracy : 27.283333333333335\n",
            "Average Model on Test | Loss : 2.1924315655231474 | Accuracy : 28.25\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 1.2803158215681711 | Accuracy : 62.541666666666664\n",
            "Average Model on Test | Loss : 1.2626827856898308 | Accuracy : 63.5\n",
            "============================================\n",
            "Local batch size : 10\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.1986783031622568 | Accuracy : 33.19\n",
            "Average Model on Test | Loss : 2.1959871423244475 | Accuracy : 34.12\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 1.7776983179648718 | Accuracy : 64.205\n",
            "Average Model on Test | Loss : 1.7665291899442672 | Accuracy : 64.79\n",
            "============================================\n",
            "Local batch size : 5\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 1.9600602146983146 | Accuracy : 56.895\n",
            "Average Model on Test | Loss : 1.9546285825967789 | Accuracy : 58.45\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 1.5067665842175484 | Accuracy : 67.52166666666666\n",
            "Average Model on Test | Loss : 1.4976004260778426 | Accuracy : 67.91\n",
            "============================================\n",
            "Local batch size : 3\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.103203803598881 | Accuracy : 52.39833333333333\n",
            "Average Model on Test | Loss : 2.1008808082342147 | Accuracy : 52.4\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 1.4985308030247688 | Accuracy : 55.92\n",
            "Average Model on Test | Loss : 1.4919696235656739 | Accuracy : 55.51\n",
            "============================================\n",
            "Local batch size : 2\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.008376398682594 | Accuracy : 48.265\n",
            "Average Model on Test | Loss : 2.000119224190712 | Accuracy : 49.51\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 0.9536210193236669 | Accuracy : 75.20666666666666\n",
            "Average Model on Test | Loss : 0.942531398832798 | Accuracy : 75.58\n",
            "============================================\n",
            "Local batch size : 1\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 1.8438863372802734 | Accuracy : 46.625\n",
            "Average Model on Test | Loss : 1.8288373053073883 | Accuracy : 47.57\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 0.8853879099090894 | Accuracy : 70.71166666666667\n",
            "Average Model on Test | Loss : 0.8695475977659225 | Accuracy : 70.5\n",
            "============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Repeat the study on CIFAR-1"
      ],
      "metadata": {
        "id": "IBvDQd54zT1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_train = datasets.CIFAR100(root=\"./data_cifar100\", train=True, download=True, transform=transforms.ToTensor())\n",
        "cifar_test = datasets.CIFAR100(root=\"./data_cifar100\", train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "class CNN2(nn.Module):\n",
        "  def __init__(self, num_classes=100):\n",
        "    super(CNN2, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(64 * 28 * 28, 256)\n",
        "    self.fc2 = nn.Linear(256, num_classes)\n",
        "    self.max_pool2d = nn.MaxPool2d(2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.relu(self.conv2(self.relu(self.conv1(x))))\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.fc2(self.relu(self.fc1(out)))\n",
        "    return out\n",
        "\n",
        "def without_common_cifar(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001):\n",
        "  sub1_load, sub2_load, dl_train, dl_test = get_subsets(mnist_train, mnist_test, nb_subset)\n",
        "\n",
        "  #Cas without initializing the common parameters donc pas de modèle partagé\n",
        "  model1 = CNN2().to(device)\n",
        "  model2 = CNN2().to(device)\n",
        "  opti1 = optim.Adam(model1.parameters(), lr=lr)\n",
        "  opti2 = optim.Adam(model2.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(local_epoch):\n",
        "    train_one_epoch(model1, sub1_load, opti1, criterion)\n",
        "    train_one_epoch(model2, sub2_load, opti2, criterion)\n",
        "  moy = average_model_parameters([model1, model2], average_weight)\n",
        "  moy_model = CNN2().to(device)\n",
        "  update_parameters(moy_model, moy)\n",
        "  train_l, train_acc = evaluate(moy_model, dl_train, criterion)\n",
        "  test_l, test_acc = evaluate(moy_model, dl_test, criterion)\n",
        "  print(\"For no common init :\")\n",
        "  print(f\"Average Model on Train | Loss : {train_l} | Accuracy : {train_acc}\")\n",
        "  print(f\"Average Model on Test | Loss : {test_l} | Accuracy : {test_acc}\")\n",
        "  return moy_model\n",
        "\n",
        "def with_common_cifar(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001):\n",
        "  sub1_load, sub2_load, dl_train, dl_test = get_subsets(mnist_train, mnist_test, nb_subset)\n",
        "\n",
        "  #Cas with initializing the common parameters\n",
        "  model = CNN2().to(device)\n",
        "  model1 = copy.deepcopy(model)\n",
        "  model2 = copy.deepcopy(model)\n",
        "  opti1 = optim.Adam(model1.parameters(), lr=lr)\n",
        "  opti2 = optim.Adam(model2.parameters(), lr=lr)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  for epoch in range(local_epoch):\n",
        "    train_one_epoch(model1, sub1_load, opti1, criterion)\n",
        "    train_one_epoch(model2, sub2_load, opti2, criterion)\n",
        "  moy = average_model_parameters([model1, model2], average_weight)\n",
        "  moy_model = CNN2().to(device)\n",
        "  update_parameters(moy_model, moy)\n",
        "  train_l, train_acc = evaluate(moy_model, dl_train, criterion)\n",
        "  test_l, test_acc = evaluate(moy_model, dl_test, criterion)\n",
        "  print(\"For no common init :\")\n",
        "  print(f\"Average Model on Train | Loss : {train_l} | Accuracy : {train_acc}\")\n",
        "  print(f\"Average Model on Test | Loss : {test_l} | Accuracy : {test_acc}\")\n",
        "  return moy_model\n",
        "\n",
        "model_no_init_cifar = without_common_cifar(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)\n",
        "model_init_cifar = with_common_cifar(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwiSkPmYzU23",
        "outputId": "72c7c67a-9b4d-48f6-9f11-efb6be3facb1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 3.1996665533383686 | Accuracy : 24.366666666666667\n",
            "Average Model on Test | Loss : 3.1919516575336457 | Accuracy : 24.01\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 1.1024152268966039 | Accuracy : 73.01833333333333\n",
            "Average Model on Test | Loss : 1.083370047211647 | Accuracy : 73.71\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for b in [50, 25, 10, 5, 3, 2, 1]:\n",
        "  print(f\"Local batch size : {b}\")\n",
        "  model_no_init_cifar = without_common_cifar(nb_subset=600, local_batch=b, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)\n",
        "  model_init_cifar = with_common_cifar(nb_subset=600, local_batch=b, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)\n",
        "  print(\"============================================\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F15l1Yc28lJ",
        "outputId": "a436ce2e-ca3c-460e-af5c-b7a970badb26"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local batch size : 50\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.814530223409335 | Accuracy : 30.088333333333335\n",
            "Average Model on Test | Loss : 2.8065655970573427 | Accuracy : 30.34\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 0.7401816385984421 | Accuracy : 80.02666666666667\n",
            "Average Model on Test | Loss : 0.7243328861892223 | Accuracy : 80.95\n",
            "============================================\n",
            "Local batch size : 25\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.3636282481749853 | Accuracy : 26.96\n",
            "Average Model on Test | Loss : 2.346817238330841 | Accuracy : 27.41\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 1.0028552916149298 | Accuracy : 73.60666666666667\n",
            "Average Model on Test | Loss : 0.9730284932255745 | Accuracy : 74.64\n",
            "============================================\n",
            "Local batch size : 10\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.641354060769081 | Accuracy : 39.17666666666667\n",
            "Average Model on Test | Loss : 2.622829726934433 | Accuracy : 39.44\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 0.941547483553489 | Accuracy : 68.15333333333334\n",
            "Average Model on Test | Loss : 0.8977361784875393 | Accuracy : 69.39\n",
            "============================================\n",
            "Local batch size : 5\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.502533021966616 | Accuracy : 31.015\n",
            "Average Model on Test | Loss : 2.4922813177108765 | Accuracy : 30.98\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 0.7058270429819822 | Accuracy : 79.555\n",
            "Average Model on Test | Loss : 0.6870279853045941 | Accuracy : 80.08\n",
            "============================================\n",
            "Local batch size : 3\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.805157779653867 | Accuracy : 43.245\n",
            "Average Model on Test | Loss : 2.7907011377811433 | Accuracy : 43.43\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 0.7825740714371204 | Accuracy : 76.54166666666667\n",
            "Average Model on Test | Loss : 0.7600486733019352 | Accuracy : 77.34\n",
            "============================================\n",
            "Local batch size : 2\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.593520377079646 | Accuracy : 31.208333333333332\n",
            "Average Model on Test | Loss : 2.5798567855358123 | Accuracy : 32.12\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 0.8375911929706732 | Accuracy : 78.93833333333333\n",
            "Average Model on Test | Loss : 0.8161741468310356 | Accuracy : 79.84\n",
            "============================================\n",
            "Local batch size : 1\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 2.7862308571736016 | Accuracy : 26.808333333333334\n",
            "Average Model on Test | Loss : 2.7751674103736876 | Accuracy : 26.0\n",
            "For no common init :\n",
            "Average Model on Train | Loss : 0.6331568018843731 | Accuracy : 81.59166666666667\n",
            "Average Model on Test | Loss : 0.6135531345009804 | Accuracy : 82.25\n",
            "============================================\n"
          ]
        }
      ]
    }
  ]
}