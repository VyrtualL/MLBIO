{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kCceAgu90vQE",
    "outputId": "bf9744ff-605a-41c4-cfe0-07b6ff4e4126"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwQQfCPi1EkW"
   },
   "source": [
    "## Load the MNIST dataset (or any other dataset like HAM 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rovmjDDW0_jO",
    "outputId": "c10c080d-f2fa-401d-8573-208bb917a855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 54.3MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.95MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 15.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.08MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhQyHGAU1v56"
   },
   "source": [
    "## Extract two subsets of 600 data points each (without intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QAN-1uL61z4-"
   },
   "outputs": [],
   "source": [
    "def get_subsets(dataset_train, dataset_test, subset_size):\n",
    "  ind = np.arange(len(dataset_train))\n",
    "  np.random.shuffle(ind)\n",
    "  #ind = ind[: 2 * subset_size]\n",
    "\n",
    "\n",
    "\n",
    "  sub1 = ind[: subset_size]\n",
    "  sub2 = ind[subset_size : 2 * subset_size]\n",
    "  true_sub1 = Subset(dataset_train, sub1)\n",
    "  true_sub2 = Subset(dataset_train, sub2)\n",
    "\n",
    "  sub1_load = DataLoader(true_sub1, batch_size=50, shuffle=True)\n",
    "  sub2_load = DataLoader(true_sub2, batch_size=50, shuffle=True)\n",
    "  dl_train = DataLoader(dataset_train, batch_size=50, shuffle=True)\n",
    "  dl_test = DataLoader(dataset_test, batch_size=50, shuffle=False)\n",
    "\n",
    "  return sub1_load, sub2_load, dl_train, dl_test\n",
    "\n",
    "#get_subsets(mnist_train, mnist_test, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0xUN0uD58Pm"
   },
   "source": [
    "## Create a simple Convolutional Neural Network (2 convolutional layers and 2 dense layers, for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zz6wA48x6A_A"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(CNN, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc1 = nn.Linear(64 * 28 * 28, 128)\n",
    "    self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.relu(self.conv2(self.relu(self.conv1(x))))\n",
    "    out = out.view(out.size(0), -1)\n",
    "    out = self.fc2(self.relu(self.fc1(out)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM1dWkIX9cvo"
   },
   "source": [
    "## Create a function average_model_parameters(models: iterable, average_weight): iterable that takes a list of models as an argument and returns the weighted average of the parameters of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1J-Pl1UV9eMj"
   },
   "outputs": [],
   "source": [
    "def average_model_parameters(models, average_weight):\n",
    "  with torch.no_grad():\n",
    "\n",
    "    mp = [list(m.parameters()) for m in models]\n",
    "    #print(mp)\n",
    "    res_amp = []\n",
    "    for i in range(len(mp[0])):\n",
    "      sum = 0\n",
    "      for j, w in enumerate(average_weight):\n",
    "        sum += w * mp[j][i].data\n",
    "      #print(sum)\n",
    "      res_amp.append(sum) #.clone() deleted\n",
    "  return res_amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THkq3V_nBI1K"
   },
   "source": [
    "## Create a function that updates the parameters of a model from a list of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VNMhmLHdBM_H"
   },
   "outputs": [],
   "source": [
    "def update_parameters(model, param):\n",
    "  with torch.no_grad():\n",
    "    for i, p in zip(model.parameters(), param):\n",
    "      i.data = p.clone()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
    "    model.train()\n",
    "    rl = 0.0\n",
    "    truee = 0\n",
    "    total = 0\n",
    "    #other = 0\n",
    "    for i, t in dataloader:\n",
    "        i = i.to(device)\n",
    "        t = t.to(device)\n",
    "        #stop ici, continuer demain, doit faire evaluate (same) + update loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(i)\n",
    "        loss = criterion(outputs, t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #faire modif\n",
    "        rl += loss.item() * i.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        truee += predicted.eq(t).sum().item()\n",
    "        total += t.size(0)\n",
    "    epoch_l = rl / total\n",
    "    epoch_acc = 100.0 * truee / total\n",
    "    return epoch_l, epoch_acc\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    rl = 0.0\n",
    "    truee = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, t in dataloader:\n",
    "            i = i.to(device)\n",
    "            t = t.to(device)\n",
    "            outputs = model(i)\n",
    "            loss = criterion(outputs, t)\n",
    "            rl += loss.item() * i.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            truee += predicted.eq(t).sum().item()\n",
    "            total += t.size(0)\n",
    "    #ajouter conditions\n",
    "    epoch_l = rl / total if total > 0 else 0\n",
    "    epoch_acc = 100.0 * truee / total if total > 0 else 0\n",
    "    return epoch_l, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3llFL-yYBqEn"
   },
   "source": [
    "## Create a script/code/function that reproduces Algorithm 1, considering that both models are on your machine. Use an average_weight=[1/2, 1/2]. Reuse the same setup as in the article (50 examples per local batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZkmrDL1QBwWR"
   },
   "outputs": [],
   "source": [
    "def without_common(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001):\n",
    "  sub1_load, sub2_load, dl_train, dl_test = get_subsets(mnist_train, mnist_test, nb_subset)\n",
    "\n",
    "  #Cas without initializing the common parameters donc pas de modèle partagé\n",
    "  model1 = CNN().to(device)\n",
    "  model2 = CNN().to(device)\n",
    "  opti1 = optim.Adam(model1.parameters(), lr=lr)\n",
    "  opti2 = optim.Adam(model2.parameters(), lr=lr)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  for epoch in range(local_epoch):\n",
    "    train_one_epoch(model1, sub1_load, opti1, criterion)\n",
    "    train_one_epoch(model2, sub2_load, opti2, criterion)\n",
    "  moy = average_model_parameters([model1, model2], average_weight)\n",
    "  moy_model = CNN().to(device)\n",
    "  update_parameters(moy_model, moy)\n",
    "  train_l, train_acc = evaluate(moy_model, dl_train, criterion)\n",
    "  test_l, test_acc = evaluate(moy_model, dl_test, criterion)\n",
    "  print(\"For no common init :\")\n",
    "  print(f\"Average Model on Train | Loss : {train_l} | Accuracy : {train_acc}\")\n",
    "  print(f\"Average Model on Test | Loss : {test_l} | Accuracy : {test_acc}\")\n",
    "  return moy_model\n",
    "\n",
    "\n",
    "def with_common(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001):\n",
    "  sub1_load, sub2_load, dl_train, dl_test = get_subsets(mnist_train, mnist_test, nb_subset)\n",
    "\n",
    "  #Cas with initializing the common parameters\n",
    "  model = CNN().to(device)\n",
    "  model1 = copy.deepcopy(model)\n",
    "  model2 = copy.deepcopy(model)\n",
    "  opti1 = optim.Adam(model1.parameters(), lr=lr)\n",
    "  opti2 = optim.Adam(model2.parameters(), lr=lr)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  for epoch in range(local_epoch):\n",
    "    train_one_epoch(model1, sub1_load, opti1, criterion)\n",
    "    train_one_epoch(model2, sub2_load, opti2, criterion)\n",
    "  moy = average_model_parameters([model1, model2], average_weight)\n",
    "  moy_model = CNN().to(device)\n",
    "  update_parameters(moy_model, moy)\n",
    "  train_l, train_acc = evaluate(moy_model, dl_train, criterion)\n",
    "  test_l, test_acc = evaluate(moy_model, dl_test, criterion)\n",
    "  print(\"For common init :\")\n",
    "  print(f\"Average Model on Train | Loss : {train_l} | Accuracy : {train_acc}\")\n",
    "  print(f\"Average Model on Test | Loss : {test_l} | Accuracy : {test_acc}\")\n",
    "  return moy_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT2h9bB-x1gn"
   },
   "source": [
    "## Train your models without initializing the common parameters and measure the performance on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YXcs8zHGx5z9",
    "outputId": "3f1a0c08-4943-49a8-ca5b-58855b627bfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For no common init :\n",
      "Average Model on Train | Loss : 1.9949518220623335 | Accuracy : 58.803333333333335\n",
      "Average Model on Test | Loss : 1.9894275826215744 | Accuracy : 59.59\n"
     ]
    }
   ],
   "source": [
    "model_no_init = without_common(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9aBknkTyM92"
   },
   "source": [
    "## Train your models with the initialization of common parameters and verify that the performance is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmYXlWhnyOOL",
    "outputId": "28713027-2c64-431f-c0b7-909a7e3a9cea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For common init :\n",
      "Average Model on Train | Loss : 1.1920624007284641 | Accuracy : 73.65666666666667\n",
      "Average Model on Test | Loss : 1.1699778446555138 | Accuracy : 75.06\n"
     ]
    }
   ],
   "source": [
    "model_init = with_common(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwryZRXrycJO"
   },
   "source": [
    "## Reduce the number of data points in each sub-batch. What is the minimum number of data points necessary for the final model to have acceptable performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDFSG7yUygMr",
    "outputId": "c2c27c0c-15fe-4ddf-f370-84feb9ab7cc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local batch size : 50\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.0872479326526325 | Accuracy : 54.23166666666667\n",
      "Average Model on Test | Loss : 2.0835875594615936 | Accuracy : 54.57\n",
      "For common init :\n",
      "Average Model on Train | Loss : 1.6214909456173578 | Accuracy : 65.68\n",
      "Average Model on Test | Loss : 1.6233508038520812 | Accuracy : 66.28\n",
      "============================================\n",
      "Local batch size : 25\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.0293833129604657 | Accuracy : 40.67666666666667\n",
      "Average Model on Test | Loss : 2.0210876554250716 | Accuracy : 41.62\n",
      "For common init :\n",
      "Average Model on Train | Loss : 1.1849098410705725 | Accuracy : 67.66166666666666\n",
      "Average Model on Test | Loss : 1.1752170172333718 | Accuracy : 68.36\n",
      "============================================\n",
      "Local batch size : 10\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.0186011198163034 | Accuracy : 43.725\n",
      "Average Model on Test | Loss : 2.015729944705963 | Accuracy : 43.16\n",
      "For common init :\n",
      "Average Model on Train | Loss : 1.0375525864462058 | Accuracy : 67.23833333333333\n",
      "Average Model on Test | Loss : 1.0272497843205928 | Accuracy : 67.09\n",
      "============================================\n",
      "Local batch size : 5\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.1445909655094146 | Accuracy : 28.921666666666667\n",
      "Average Model on Test | Loss : 2.1403702342510225 | Accuracy : 28.99\n",
      "For common init :\n",
      "Average Model on Train | Loss : 1.2514371641973654 | Accuracy : 66.15666666666667\n",
      "Average Model on Test | Loss : 1.2349790206551552 | Accuracy : 68.24\n",
      "============================================\n",
      "Local batch size : 3\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.0984937989711763 | Accuracy : 41.4\n",
      "Average Model on Test | Loss : 2.0979101771116255 | Accuracy : 41.05\n",
      "For common init :\n",
      "Average Model on Train | Loss : 1.6449357463916143 | Accuracy : 65.68\n",
      "Average Model on Test | Loss : 1.6266943717002869 | Accuracy : 67.24\n",
      "============================================\n",
      "Local batch size : 2\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.073375807007154 | Accuracy : 60.781666666666666\n",
      "Average Model on Test | Loss : 2.068513239622116 | Accuracy : 62.12\n",
      "For common init :\n",
      "Average Model on Train | Loss : 1.498138258755207 | Accuracy : 53.485\n",
      "Average Model on Test | Loss : 1.4899924880266189 | Accuracy : 53.99\n",
      "============================================\n",
      "Local batch size : 1\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.001790478229523 | Accuracy : 46.538333333333334\n",
      "Average Model on Test | Loss : 1.9920908170938492 | Accuracy : 47.0\n",
      "For common init :\n",
      "Average Model on Train | Loss : 1.156958496371905 | Accuracy : 71.17166666666667\n",
      "Average Model on Test | Loss : 1.1472931241989135 | Accuracy : 71.73\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "for b in [50, 25, 10, 5, 3, 2, 1]:\n",
    "  print(f\"Local batch size : {b}\")\n",
    "  model_no_init = without_common(nb_subset=600, local_batch=b, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)\n",
    "  model_init = with_common(nb_subset=600, local_batch=b, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)\n",
    "  print(\"============================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBvDQd54zT1m"
   },
   "source": [
    "## Repeat the study on CIFAR-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zwiSkPmYzU23",
    "outputId": "11e21927-39f0-4211-ab6e-ade574fa24a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data_cifar100/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:02<00:00, 74.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data_cifar100/cifar-100-python.tar.gz to ./data_cifar100\n",
      "Files already downloaded and verified\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.59198892335097 | Accuracy : 43.38666666666666\n",
      "Average Model on Test | Loss : 2.5748534286022187 | Accuracy : 43.6\n",
      "For common init :\n",
      "Average Model on Train | Loss : 0.7847658570607503 | Accuracy : 77.87333333333333\n",
      "Average Model on Test | Loss : 0.7622268906235695 | Accuracy : 78.25\n"
     ]
    }
   ],
   "source": [
    "cifar_train = datasets.CIFAR100(root=\"./data_cifar100\", train=True, download=True, transform=transforms.ToTensor())\n",
    "cifar_test = datasets.CIFAR100(root=\"./data_cifar100\", train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "class CNN2(nn.Module):\n",
    "  def __init__(self, num_classes=100):\n",
    "    super(CNN2, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "    self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc1 = nn.Linear(64 * 28 * 28, 256)\n",
    "    self.fc2 = nn.Linear(256, num_classes)\n",
    "    self.max_pool2d = nn.MaxPool2d(2)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.relu(self.conv2(self.relu(self.conv1(x))))\n",
    "    out = out.view(out.size(0), -1)\n",
    "    out = self.fc2(self.relu(self.fc1(out)))\n",
    "    return out\n",
    "\n",
    "def without_common_cifar(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001):\n",
    "  sub1_load, sub2_load, dl_train, dl_test = get_subsets(mnist_train, mnist_test, nb_subset)\n",
    "\n",
    "  #Cas without initializing the common parameters donc pas de modèle partagé\n",
    "  model1 = CNN2().to(device)\n",
    "  model2 = CNN2().to(device)\n",
    "  opti1 = optim.Adam(model1.parameters(), lr=lr)\n",
    "  opti2 = optim.Adam(model2.parameters(), lr=lr)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  for epoch in range(local_epoch):\n",
    "    train_one_epoch(model1, sub1_load, opti1, criterion)\n",
    "    train_one_epoch(model2, sub2_load, opti2, criterion)\n",
    "  moy = average_model_parameters([model1, model2], average_weight)\n",
    "  moy_model = CNN2().to(device)\n",
    "  update_parameters(moy_model, moy)\n",
    "  train_l, train_acc = evaluate(moy_model, dl_train, criterion)\n",
    "  test_l, test_acc = evaluate(moy_model, dl_test, criterion)\n",
    "  print(\"For no common init :\")\n",
    "  print(f\"Average Model on Train | Loss : {train_l} | Accuracy : {train_acc}\")\n",
    "  print(f\"Average Model on Test | Loss : {test_l} | Accuracy : {test_acc}\")\n",
    "  return moy_model\n",
    "\n",
    "def with_common_cifar(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001):\n",
    "  sub1_load, sub2_load, dl_train, dl_test = get_subsets(mnist_train, mnist_test, nb_subset)\n",
    "\n",
    "  #Cas with initializing the common parameters\n",
    "  model = CNN2().to(device)\n",
    "  model1 = copy.deepcopy(model)\n",
    "  model2 = copy.deepcopy(model)\n",
    "  opti1 = optim.Adam(model1.parameters(), lr=lr)\n",
    "  opti2 = optim.Adam(model2.parameters(), lr=lr)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  for epoch in range(local_epoch):\n",
    "    train_one_epoch(model1, sub1_load, opti1, criterion)\n",
    "    train_one_epoch(model2, sub2_load, opti2, criterion)\n",
    "  moy = average_model_parameters([model1, model2], average_weight)\n",
    "  moy_model = CNN2().to(device)\n",
    "  update_parameters(moy_model, moy)\n",
    "  train_l, train_acc = evaluate(moy_model, dl_train, criterion)\n",
    "  test_l, test_acc = evaluate(moy_model, dl_test, criterion)\n",
    "  print(\"For common init :\")\n",
    "  print(f\"Average Model on Train | Loss : {train_l} | Accuracy : {train_acc}\")\n",
    "  print(f\"Average Model on Test | Loss : {test_l} | Accuracy : {test_acc}\")\n",
    "  return moy_model\n",
    "\n",
    "model_no_init_cifar = without_common_cifar(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)\n",
    "model_init_cifar = with_common_cifar(nb_subset=600, local_batch=50, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7F15l1Yc28lJ",
    "outputId": "0fbc0143-1ea1-47e6-f1f6-1bba46fad113"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local batch size : 50\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.9354674555857976 | Accuracy : 14.881666666666666\n",
      "Average Model on Test | Loss : 2.92389852643013 | Accuracy : 15.12\n",
      "For common init :\n",
      "Average Model on Train | Loss : 0.890872001449267 | Accuracy : 72.49\n",
      "Average Model on Test | Loss : 0.8610889618098736 | Accuracy : 73.98\n",
      "============================================\n",
      "Local batch size : 25\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.7401226967573167 | Accuracy : 12.203333333333333\n",
      "Average Model on Test | Loss : 2.7304976081848142 | Accuracy : 12.18\n",
      "For common init :\n",
      "Average Model on Train | Loss : 1.0755546415348847 | Accuracy : 71.755\n",
      "Average Model on Test | Loss : 1.0394791653752327 | Accuracy : 74.15\n",
      "============================================\n",
      "Local batch size : 10\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.396014761328697 | Accuracy : 37.97666666666667\n",
      "Average Model on Test | Loss : 2.387625995874405 | Accuracy : 37.98\n",
      "For common init :\n",
      "Average Model on Train | Loss : 0.9724938650429249 | Accuracy : 69.765\n",
      "Average Model on Test | Loss : 0.946089378297329 | Accuracy : 71.31\n",
      "============================================\n",
      "Local batch size : 5\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.5711462980508806 | Accuracy : 21.41\n",
      "Average Model on Test | Loss : 2.553798187971115 | Accuracy : 21.96\n",
      "For common init :\n",
      "Average Model on Train | Loss : 1.0341936362783115 | Accuracy : 78.00333333333333\n",
      "Average Model on Test | Loss : 1.0128144121170044 | Accuracy : 78.48\n",
      "============================================\n",
      "Local batch size : 3\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.143222145239512 | Accuracy : 32.653333333333336\n",
      "Average Model on Test | Loss : 2.1398972910642624 | Accuracy : 31.92\n",
      "For common init :\n",
      "Average Model on Train | Loss : 0.9308573357760906 | Accuracy : 75.33833333333334\n",
      "Average Model on Test | Loss : 0.9040420980751515 | Accuracy : 76.45\n",
      "============================================\n",
      "Local batch size : 2\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.4897351507345835 | Accuracy : 47.15\n",
      "Average Model on Test | Loss : 2.4666453123092653 | Accuracy : 48.42\n",
      "For common init :\n",
      "Average Model on Train | Loss : 0.8557414883871873 | Accuracy : 76.17666666666666\n",
      "Average Model on Test | Loss : 0.8353865103423596 | Accuracy : 76.94\n",
      "============================================\n",
      "Local batch size : 1\n",
      "For no common init :\n",
      "Average Model on Train | Loss : 2.9483024100462596 | Accuracy : 44.56333333333333\n",
      "Average Model on Test | Loss : 2.937803509235382 | Accuracy : 44.13\n",
      "For common init :\n",
      "Average Model on Train | Loss : 0.8285894368588924 | Accuracy : 75.605\n",
      "Average Model on Test | Loss : 0.8067474003136158 | Accuracy : 76.6\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "for b in [50, 25, 10, 5, 3, 2, 1]:\n",
    "  print(f\"Local batch size : {b}\")\n",
    "  model_no_init_cifar = without_common_cifar(nb_subset=600, local_batch=b, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)\n",
    "  model_init_cifar = with_common_cifar(nb_subset=600, local_batch=b, local_epoch=1, average_weight=[0.5, 0.5], lr=0.001)\n",
    "  print(\"============================================\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
